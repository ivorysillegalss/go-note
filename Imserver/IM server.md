# IM server

**此md记录学习im相关的一些笔记**



确保 QOS2 + 时序性

- 可达性

- 准确性

- 时序性 消息按照时间顺序排列

关于字符流和字节流：

字符流可读 但是其效率不如字节流 传输的时候不需要转化

字节流在传输的时候需要进行序列化等操作 但是其传输效率高

小公司（几百QPS）一般使用的是字符流多

但是大规模项目一般使用字节流 且protobuf





### TCP & UDP

TCP面向**连接**协议，可分为长连接或短连接。

UDP面向**消息**协议，对于每一条信息都将创建一次UDP连接。

TCP在经过三次握手之后，确认连接的建立。再将信息发送出去。并在信息确认发送出去之后返回类似ACK的信息。表示信息成功传输

UDP的每一次连接都是对于信息而言的。每一条信息都是由发送方，发送出去就不管了。并且没有相比之下精密的握手等规定。

相比之下：

UDP在弱网环境下表现更优，由于特性决定了它的消息请求体相比之下会更小。而弱网的环境下，可以将UDP原有协议，进行一定程度上的包装和自定义。从而使他更加适合IM的使用环境。



IM中，数据通讯层无论用的是UDP还是TCP协议，都同样需要消息送达保证（即QoS）机制，原因在于IM的通信是A端-Server-B端的3方通信，而非传统C/S或B/S这种2方通信







## 架构设计

### 关于消息的一致性思考

> 设两人一对一聊天，发送方A依次发出了msg1、msg2、msg3三条消息给接收方B，这三条消息该怎么保证显示时序的一致性（发送与显示的顺序一致）？ 我们知道，发送方A依次发出的msg1、msg2、msg3三条消息，到底服务端后，再由服务端中转发出时，这个顺序由于多线程的网络的问题，是有可能乱序的。 
>
> 
>
> 会出现与发出时的消息时序不一致问题（收到的消息顺序是：msg3、msg1、msg2）。 不过，实际上一对一聊天的两个人，并不需要全局消息时序的一致（因为聊天只在两人的同一会话在发生），只需要对于同一个发送方A，发给B的消息时序一致就行了。 常见优化方案，在A往B发出的消息中，加上发送方A本地的一个绝对时序（比如本机时间戳），来表示接收方B的展现时序。 那么当接收方B收到消息后，即使极端情况下消息可能存在乱序到达，但因为这个乱序的时间差对于普通用户来说体感是很短的，在UI展现层按照消息中自带的绝对时序排个序后再显示，用户其实是没有太多感知的。

上述的问题描述的是在**点对点聊天中**，消息的显示顺序和发送顺序的一致性（发送的过程中会因**多线程or网络延迟**等多方面原因有可能导致消息的乱序）。

对此场景下，可以在每条聊天信息中都附加一个标记（时间戳）。并根据此时间戳重新排序信息。

但是在此之中也会存在几个问题，譬如说，排序本身的效率不是很高。如何定义一个合适的时间段（一个合适的窗口）。并将在此时间段内的如何排序呢？

换句话来说，也就是。假设现在为了确保时序性，需要如上方解法采用一个MQ或其他容器来存储其中的信息。

- 但是如何规定这个容器的大小呢？ （一个容器一般情况下，最多存多少数据）

- 在消息间延迟低于什么值的时候，将值存到容器当中呢？（将延迟为多少的数据存入容器）

但是也许这些问题是根据IM实际使用场景所定义的，没有准确或否的答案。只有相关的优化方法。



> ### User
>
> 那么在这种情况下 是不是需要设计消息排序的时间段 规定一个进行排序的延迟范围 或是有其他更优的排序解法 并且当信息量很大的时候 这一个排序的时间成本也是很高的
>
> 
>
> ### ChatGPT
>
> 确实，当消息量增大或网络状况不稳定时，依靠时间戳进行消息排序可能会带来一些额外的挑战和考虑。以下是几种改进的方法和策略，以及如何处理大量消息时的排序效率问题：
>
> #### 设计消息排序的时间窗口
>
> 1. **定义时间窗口**：
>    - 可以设定一个时间窗口，允许在这个窗口期内接收的消息被缓存并排序。这个窗口可以是几秒钟，也可以根据网络延迟的实际情况进行调整。
>    - 时间窗口的目的是在不影响用户体验的前提下，给予系统足够的时间来接收和排序可能存在的乱序消息。
> 2. **动态调整**：
>    - 根据实际的网络状况和历史数据，动态调整时间窗口的大小。例如，如果检测到消息的平均延迟增加，可以相应地增加窗口大小。
>
> #### 排序算法优化
>
> - 高效的排序算法
>
>   - 对于大量的消息排序，可以使用更高效的排序算法，比如快速排序、归并排序等，这些算法在最坏的情况下也能提供较好的性能。
>- 在软件设计中，可以采用优先队列（例如使用二叉堆实现）来管理这些消息，优先队列可以高效地插入新消息并取出最早的消息。
> 
> #### 采用序列号而非时间戳
>
> - 序列号系统
>
>   - 另一个选择是使用一个增长的序列号而非时间戳。每发送一条消息，序列号增加。这样，即便两个系统的时钟不同步，消息的顺序也能被正确处理。
>- 这要求发送方在每次会话开始时同步序列号，确保消息的连贯性。
> 
>#### 实时同步和校正机制
> 
> - **时钟同步**：
>  - 如果继续使用时间戳，那么可以在客户端之间实施一种时钟同步机制，如NTP（网络时间协议），来减少时间差异带来的影响。
> - **消息确认和重发机制**：
>  - 设计一个机制来确认消息的接收状态，并在未确认的情况下进行消息重发。这不仅帮助保持消息顺序，还确保了消息的可靠传输。
> 
> #### 总结
> 
> 在设计这样的系统时，关键在于找到实现效率和用户体验之间的平衡。选择合适的时间窗口、优化排序算法、使用序列号和实施时间同步策略都是提高系统性能和用户满意度的有效方法。在实际应用中，可以根据特定的需求和条件选择最适合的策略。





### 全局时序性 & 单点时序性

确实，全局时序性和单个群聊的时序性在概念和实际应用中有所不同。了解这两者的差异对于设计聊天系统尤为重要，特别是在考虑消息的一致性和用户体验时。

#### 全局时序性（Global Ordering）

全局时序性是指在一个系统中，所有消息都遵循一个统一的、全系统范围内的时序。这种时序保证了来自不同发送者的所有消息都按照一定的全局顺序被处理和接收。全局时序性常见于需要严格顺序处理的系统，比如金融交易系统，其中交易的顺序可能影响最终结果。

- **应用场景**：例如，一个大型的社交平台可能需要维护不同聊天群组之间消息的全局时序，以便于进行系统范围内的搜索、审核或其他全局操作。
- **挑战**：实现全局时序性通常要求复杂的时间同步机制和一致性协议，可能涉及较高的技术复杂度和资源消耗。

##### 单个群聊的时序性（Local Ordering）

单个群聊的时序性指的是在同一个聊天群组内，确保消息按发送顺序被所有成员看到的一致性。这种时序性只关注单一聊天环境中的消息顺序，不涉及跨群组或系统的消息顺序。

- **应用场景**：任何群聊系统，如WhatsApp、Telegram、企业内部通讯工具等。用户期望在同一个聊天群组内，无论是哪个成员发送的消息，都应该按照发送的顺序展现。
- **实现**：通常通过给每个消息加上时间戳或序列号来实现。只要群组内的成员接收和处理消息的机制一致，就可以保证群内消息的时序性。

##### 关键差异

- **范围与复杂性**：全局时序性需要在整个系统的层面上维护消息顺序，技术实现更为复杂；而单个群聊的时序性只关注局部环境，实现相对简单。
- **性能考虑**：全局时序性可能因为涉及更广泛的数据同步和一致性检查，对系统性能有较大影响；单个群聊的时序性对性能的影响较小，更容易优化。
- **用户体验**：从用户的角度看，群聊的时序性直接影响到他们的交流体验，而全局时序性通常是后台操作，用户不易察觉。







### 关于消息可用性 对上方一致性的具体实现方案

**消息可用性 = 可靠性 + 时序性**

#### 上行信息

为了保证信息的不重不漏，最基本的需要客户端发送信息到服务器之后，服务器返回一个ACK的信息。

但是如上所说，由于多线程的处理和网络延迟的因素，会带来很多不可预测的结果。

而为了在此环节中保证消息的完整性，最为重要的是**标志信息顺序的 唯一标识符** 

并且这个唯一标识符的设计，需要保证信息的**先后顺序**，并且性能需要足够高，保证消息处理的时间延迟足够低的基础上，尽量减少空间上的占用。

而 **服务器 & 客户端A** 的消息传递，可以分为sessionID和clientID。每一次长连接都可以认为是一次会话，而每一次会话中的clientID都会重置，并且也将是独立的。

**（clientID的生命周期只存在与一条信息的上行or下行当中）**

在此可以分为三种方案：

> ### 方案一：严格递增的clientID
>
> #### 工作机制：
>
> 1. **连接建立**：客户端A与服务器建立连接，clientID初始化为0。
> 2. **clientID分配**：每次发送消息时，clientID严格递增。
> 3. **服务器验证**：服务器检查接收到的clientID是否为上一个preClientID加1。如果是，接受消息；如果不是，拒绝消息。
> 4. **消息确认**：服务器仅在成功接收消息后发送ACK确认给客户端A。
> 5. **客户端行为**：客户端A在收到服务器的ACK后，停止重发该消息；否则，可以重发消息，最多三次。
>
> #### 特点：
>
> - **顺序性强**：通过严格的递增确保了消息的顺序。
> - **重发机制**：有明确的重发策略，便于处理消息丢失的情况。
> - **简单性**：基于数字递增的机制相对简单和直观。
>
> 
>
> ### 方案二：使用时间戳的clientID链表
>
> #### 工作机制：
>
> 1. **clientID生成**：客户端A使用本地时间戳作为clientID。
> 2. **链表机制**：客户端在发送消息时，同时携带上一条消息的clientID。
> 3. **服务器验证**：服务器存储上一条消息的clientID作为preClientID，并将其与当前消息的preClientID进行比较。只有当它们匹配时，才接受消息。
> 4. **消息处理**：如果消息未按预期到达或preClientID不匹配，服务器拒绝该消息。
>
> #### 特点：
>
> - **基于时间的ID**：使用时间戳可以避免ID冲突，但可能面临时间同步的问题。
> - **链表依赖性**：消息的接收依赖于前一消息的ID，这可能在丢包较多的网络环境中导致连锁问题。
> - **复杂性**：需要处理更多的异常情况，如时间戳冲突和消息丢失的连锁反应。
> - **内存消耗**：当消息数量极大时，以时间戳作为clientID与preClientID会给服务器带来不小的压力。
>
> 
>
> ### 本质区别：
>
> - **ID生成方式**：方案一中ID严格递增，简单明了；方案二中使用时间戳，可能受到系统时间不同步的影响。
> - **容错性**：方案一通过重发机制处理消息丢失，方案二依赖于消息链的完整性，对单个消息丢失更敏感。
> - **实现复杂度**：方案二在实现上可能更复杂，需要处理时间戳相关的问题和更复杂的状态同步问题。



由于这种处理方式更使用于单个客户端和服务器之间的连接，并且其**确保消息合法性（时序性）**的条件更为严苛，所以在这一场景下，一旦网络等不可知因素出现波动。TCP出现超时重传的概率是大大增加的。这一情况会给服务器带来非常大的压力。

并且当某一次消息传达失败后。由于消息依赖的clientID与预期不符，不仅丢失的信息无法被处理，其后所以依赖该消息的消息也会被阻塞or拒绝，导致消息链断裂。



当因素干扰而导致消息乱序时，上方两种方案中信息均需重传。客户端重新发送信息、服务器验证消息时序性......等等等。可以想到如果能**暂时存储乱序发来的信息，并将其重排序，便可节省大量的成本**。

于是我们可以在此基础上推出方案三

> ### 方案三：在原基础上设计clientID list 
>
> 1.服务端针对每个连接存储多个clientID,形成clientID list 
>
> 2.使用此client List作为滑动窗口，来保证消息幂等
>
> 
>
> #### 滑动窗口机制的内存开销
>
> 1. **存储每个clientID**：每个连接需要服务器存储一系列clientID，这些ID标识了已接收和待接收的消息。这个列表或窗口随着每条新消息的接收而更新。
> 2. **时间戳作为clientID**：使用时间戳作为clientID时，通常需要更多的内存来存储（例如，使用64位来存储精确到微秒的时间戳）。此外，时间戳的范围和精度更高，可能导致列表存储更多的唯一值。
> 3. **滑动窗口的大小和管理**：窗口的大小（即可以存储多少个clientID）需要根据应用的具体需求和网络条件调整。较大的窗口可能提供更高的容错性和灵活性，但同时也需要更多的内存。
> 4. **高并发和高吞吐量环境**：在高并发连接和高消息吞吐量的环境中，服务器可能需要同时管理多个活跃连接的clientID list，这会进一步增加内存需求。

虽然效率有所提升，但是这种方案仍相当于是一种空间换时间的举措。需要考虑到内存开销相关的问题。

当然我们可以通过限制窗口大小，根据im的具体使用场景更改为更高效的数据结构等做法。





#### 消息转发

假设现在信息已经正常从客户端A发到服务器当中，并且已正确给客户端返回了ACK信息。称为消息正常地上行到服务器当中。



**如何准确判断不同客户端发送信息的先后顺序？**



此时我们需要考虑的问题是信息的顺序以及入库的问题。已知对于单个客户端来说，我们可以根据分配的**clientID**来判断消息的前后顺序。但是这个clientID只是对单个客户端而言的，仅仅保证了发送方的消息顺序，并不能保证整个会话的消息顺序。整个单聊or群聊的消息顺序仍需重新讨论。但是这一优先级其实并没有同一个客户消息的时序性优先级高。

为了保证消息的顺序性，我们需要重新定义一个**sqlID**，**代表在当前的会话环境中整体消息的顺序**。于是我们可以得到以下传递性的关系。

```
// 设两条信息clientID分别为cid1 cid2
if cid2 > cid1 :
	sid2 > sid1
// 并且此时若有来自其他客户端的信息时 ,sid2 - sid1 != cid2 - cid1
```

关于seqID还需要做一些额外的解释：

- 这个sqlID是针对某一特定会话的而非全局的，即针对特定的群聊or单聊。
- 若设定每一个不同的群组或小窗都有自身的**sessionID**，则在每一个sessionID下都有一组seqlID。

- 在上方所有条件的支持下，可以使用int64的数据类型代表seqID。

搞清楚了seqID，现在需要对他的存储以及分配作分析：

先从最简单的暴力解法开始考虑，此时seqID是面向全局而言的，无需重置。我们可以往redis中存储一个key，名为`msgincrby`，存储了当前最大的seqID。但是这一做法，所有群聊中的消息推送都会打在redis上，对redis的压力很大。非常不现实。

于是我们就优化出了已有的seqID的方案。每个会话session都分配不同的sqlID。此时存储的结构更加清晰并且压力更小。我们可以根据已有的sessionID与seqID进行拼接，如设置redis中的key为`msg:{sessionID}:seqID`其中sessionID使用特定群聊编号替换。并且此时的seqID也可以使用int64来进行存储。从特定的应用场景上进行分析，一个活跃的群聊，同一时刻信息推送的数量也是此时redis可接受的。

同时这一种组合Key的方法也便于使用哈希分片来进行**水平拓展**

> ### 哈希分片
>
> - **哈希函数**：系统对组合Key应用哈希函数，生成一个哈希值。哈希函数的设计需确保均匀分布，以避免某些节点比其他节点承担更多负载。
> - **分片逻辑**：根据哈希值，系统将消息定向到特定的节点或分片。这通常是通过取模操作实现的，例如，使用`hash(key) % N`，其中`N`是节点或分片的数量。
> - **水平扩展**：当系统需要扩展时，可以增加更多的节点或分片。消息可以根据其哈希值重新分配到新的节点，从而实现负载的重新平衡。
>
> ### 水平扩展的挑战与解决策略
>
> - **一致性哈希**：在传统的哈希分片中，添加或移除节点需要重新哈希所有的Key，这可能导致大规模的数据迁移。使用一致性哈希可以减少这种影响，因为它只影响到直接相邻的节点。
> - **数据迁移**：在节点增减时，确保数据迁移的过程中服务的高可用和数据的完整性是关键。这通常涉及到复杂的数据同步和故障转移机制。

综上，我们得出了一个较为合理的分配并且的方法。

然后，我们需要对消息的存储进行分析。结合sqlID的分配方法，我们可以得到在一个session中信息的唯一标识符。同样通过拼接两者，然后经过算法的处理，我们也可以得到一个唯一标识信息的msgID。接下来只需将此msgID异步交给MQ存入DB即可。



以上便是在正常运行的前提下，消息转发的运行逻辑。同时，我们也需要考虑失败的情况。

从运行的逻辑上来说：

1. 假如在根据sessionID分配seqID的时候，请求已失败或进程崩溃

> 消息此时未成功上传到服务器当中，不回复ACK信息，记为信息转发失败

2. 存储seqID的redis故障，如何处理？

> redis主从同步，主节点拉取从节点的值
>
> 但是拉取的过程和分配seqID是**异步**的过程
>
> 有可能在同步的时候，主节点的ID值已经更新，两个节点的值矛盾。
>
> 如果不进行处理的话。同一个seqID会分配给两个信息，此时造成消息的**回退**
>
> 对整个逻辑都会产生影响
>
> 
>
> 解决这个问题
>
> 我们可以在redis同时存储一个nodeID 代表是当前redis的节点编号 **（LUA）**
>
> 每次在拉取和分配的时候，将拉取的节点编号与存入的redis节点的编号进行比较。此处不赘述，为LUA脚本作用
>
> 如果比较的值存在矛盾，代表着此时出现了回退的问题，将此时的seqID值进行**跳跃（消息跳变）**，虽然对部分的资源进行了浪费（**消息漏洞**），但是保证了消息的可用性。
>
> 在客户端分析客户端下行信息的时候也需要考虑这一问题（**消息补漏**），下文介绍

3. 通过MQ写入历史记录的时候，MQ崩溃

> 结合1，我们可以再次调整回复ACK的时机，在成功存储消息之后才回复ACK。
>
> 但是此时seqID已经分配，会造成seqID的浪费。同样会造成**消息跳变**的问题，需要进行**消息补漏**



通过上方的对seqID的讨论，虽然经过优化，但是未免seqID仍不能做到**严格递增**，本质上是**趋势递增**



综上，整个消息转发的过程我们可以分为

1. 获得上行信息
2. 从redie中根据sessionID分配seqID，更新redis中seqID末值
3. 将信息的seqID和sessionID拼接，进行算法编码，发送给MQ，准备存入DB
4. MQ获得信息及ID，将其存入DB当中
5. 成功存入DB，返回ACK信息



**Q:但是，虽然这一做法保证了消息的可用性，但是每次写完DB再ACK，效率不会比较低吗？可以在MQ中进行备份等操作，从而保证信息能写入DB，而在分配好seqID发给MQ的时候就返回ACK吗？这样子效率应该更高。**



#### 下行消息

若信息在服务器转发成功。需要主动将信息发送到客户端B当中。

客户端B需要判断这一信息属于哪一个session并且是否符合消息顺序。在服务器信息转发的时候，我们尽量保证了信息的严格递增，但仍有部分的情况存在问题（一般只有在redis主从切换所产生的信息漏洞）。这一部分需要进行特殊的讨论。

于是在一般聊天情境下，我们可以采用类似上行消息中`preSeqID = seqID + 1`，保证信息的时序性。当遇到消息漏洞时，客户端主动向服务器pull拉取漏洞间的信息。

> eg：seqID由于redis主从切换从1跳到了10，所以服务器在发送seqID为1的信息下一条便是seqID为10的信息。此时客户端B主动拉取1-10间的信息，服务器补充空信息（此过程称为消息补漏）



同时在当客户端B突然需要向服务器请求大量信息的时候。可以结合两种除原方案外的解决方案：

1. 服务器告诉客户端B需要申请一次http的短链接，经过短链接传输数据的信息。
2. 采用TCP的拆包，将原有的信息拆成多个小包进行发送。





### 长连接网关设计



**回顾上方的消息可用性相关的分析，我们再次澄清一下。im的场景是一个三方通信，先由客户端A上行信息给服务器，服务器下行信息给客户端B。**



作为一个imserver，其收发信息频率极高。

在消息协议上，一般情况上以**TCP的长连接**为基础，弱网环境或极端情况下可在UDP等协议的基础上进行优化。

> **为什么选择TCP长连接？**
>
> TCP是面向连接而建立，在**收发频率高**的前提下。使用TCP短连接或其他方式，且不论安全性如何，会不断的重复建立并且删除连接。这一做法非常的影响效率，而TCP长连接在大多数情况下，只需要向连接的对方（Server / Client）发送心跳数据包，便可保持连接的活跃性。在传输信息的时候也可跨越复杂的步骤，提高消息的时序性。
>
> 但是使用这一方法，消息的**安全性**不可避免的会降低。需要通过其他措施进行保证。



但是TCP的长连接是脆弱的。连接及其所携带的信息都需要保证稳定性，当用户进入电梯（弱网环境），地铁（弱网＋切换基站）。在这一些情况下，连接都会断开并重建，长连接传输中的信息也会丢失，用户的体验也会被影响。

同时，业务需求的上线，也需要重启所有的长链接。就会导致其重新频率高，稳定性低。



**此处先根据信息的处理逻辑捋一遍。**

> #### 当客户端初始化建立长链接时
>
> 1. 向某个IP的长连接服务发送创建连接信令。
> 2. 网关server解析信令得知其为创建连接信令。
> 3. 网关server,获得底层socket的FD,以及用户的uid/did,建立注册表。
> 4. 回复客户端连接建立成功。
>
> #### 当客户端发送消息时
>
> 1. 客户端发送上行消息信令。
> 2. 网关服务接收到消息，并解析信令为上行消息信令。
> 3. 根据clientID和sessionID进行路由，分配seqID等状态更新逻辑。
> 4. 然后转发给业务层服务处理，确认业务层收到消息后立即回复客户端
> ACK.
>
> #### 当业务处理后，将消息转发给接收客户端时
>
> 1. 业务根据sessionID定位到该会话的接收者的连接在哪一个网关服务上。
> 2. 然后将消息通过RPC交给网关服务，网关拿到数据后通过uid对应connID,确定fd.
> 3. 然后根据fd找到对应的socket,将消息拼接固定消息头发送给接收方客户端。
>
> #### 当连接断开的时候
>
> 1. 心跳超时，连接断开/异常断开
> 2. 状态回收释放

如下图

![image-20240501232350957](.\images\1)

上方是根据消息的处理逻辑所作出的基本图示.

但是从细节上进行考虑,我们需要考虑:



#### 信息监听及推送

为了达成监听和推送的目的,我们可以先得出如下方的一个暴力解,每一轮的收发消息需要用到三个协程,分别为**监听**,**推送**,**判断消息可达或否**

![image-20240502001938656](.\images\image-20240502001938656.png)

在此基础上,我们可以使用**go-select**语句进行轮询.并且将协程**池化**.

前者节省了协程的部分开销；后者减少了调度开销(创建和删除协程)

但是无论如何设计，若使用`go协程 + 轮询`的模式仍会存在很大一部分的成本

有没有办法可以将其修改为事件驱动呢？

可以通过修改为`reactor模型+epoll`的模式，将收发信息完全事件化。不再会因为协程堵塞产生开销。配合池化思想：

- 当epoll发出读请求时，从协程池中取信息转发

- 当逻辑处理完毕下行消息，从协程池中取协程push信息。

架构如下：

![image-20240502015814360](.\images\image-20240502015814360.png)



在了解了如何将信息的时空成本控制在最小的程度下，我们还需要考虑长连接中，如何存储长连接的状态。

#### 信息状态存储

了解如何存储状态的信息，我们需要先捋清楚有什么状态信息需要存储

- uid/did/fd 用户标识符，设备标识符，存储的文件描述符
- sessionID / connID 通讯场景标识符，连接信息
- seqID / clientID 信息时序性标识符
- connID 对应的 conn对象 **定时器存储已达or未达信息（定时器）**

**优化方式:**

- 此处定时器的作用在于判断信息是否到达，若信息到达并返回ack则代表成功到达。未到达则需要重发。在满足这一要求的情况下，我们可以使用时间轮算法，将其区间化，在某一段时间（单位为ms）内，仍未收到ack信息的消息全部重发。这一算法在某种程度上也节约了push信息时所耗费的资源.**(true?)**



- 维护uid/did到connect对象所在机器endpoint倒排:

  > 建立一个反向索引表，将用户ID(uid)和设备ID(did)与连接对象所在的终端设备IP地址加端口号建立对应关系。也就是说，可以根据uid/did查找到对应的连接对象所在的机器的网络地址(endpoint)。业务层server调用网关服务的时候，需要通过uid等信息，进行反查查询该用户目前在哪台网关服务器上



- 判断消息可达或否的**飞行队列**使用redis-list存储，进一步减少内存消耗。



上方在经过存储结构和协程的优化后，在单体架构上已经接近瓶颈。但是我们可以将他拆分为微服务。将其分为两个模块。分别是`gateway`和`state server`。拆分架构如下：

![image-20240502150257982](.\images\image-20240502150257982.png)

1. 网关serverf仅维护connID到fd的map映射
2. connect对象中定时器，飞行队列等状态交给完全独立的state server维护，与网关server之间通过RPC进行通信
3. 业务层将消息发送给state server由其控制收发逻辑

就相当于把有状态服务全部丢给state server进行存储和维护。而gateway层只负责收发。这么做虽然会因为rpc调用产生部分的时间成本。但是将有状态服务都从网关层隔离出来，减少它的业务的耦合度，提高它的可靠性。





### 领域驱动设计 DDD

domain drive design

基本概念

- 领域专家：产品&运营&顾问，对该业务领域具有深刻认识的人，掌握足够的领域知识的人。
- 统一语言：领域专家和开发团队等等角色之间对关键概念建立统一的定义，消除沟通歧义。
- 界限上下文：界限是一种判断标准，上下文强调的是对知识的隐藏，可以把微服务等价于界限上下文，一个界限上下文划分出一个自治单元，具有最小完备，自我屋行，独立进化，稳定空间四个特点，因此界限上下文对应着一个套微服务，也对应一个技术团队。
- UP(U)上游：特指被调用者，提供能力的被依赖方。
- Dow(D)下游：特指调用者，使用上游能力解决问题的角色.
- 上下文映射：表示上下文之间的依赖关系，也反映了团队直接的协作关系。
- 实体：具有唯一标示符的对象。
- 值对象：不具有唯一标示符需要依赖实体存在的对象。
- 聚合：持有一些相互强协同的实体之间的引用，通过标识符引用实体。
- 聚合根：唯一能被对外引用的句柄，任何外部访问聚合内的实体，都需要通过聚合根对象，其他聚合也仅能通过聚合根引用其他聚合。
- 工厂：就是工厂模式，用来创建并管理复杂聚合的创建与销毁。
- 资源库：负责统一管理一个聚合的增删改查行为以及状态的变更，工厂和资源库实现对聚合对象生命周期的管理。
- 领域服务：业务功能需要多个聚合之间相互协作才能完成，在聚合之上需要一个服务来封装聚合之间的协作行为。
- 应用服务：领域服务关注整体业务的横向能力建设，应用服务关注的是对横向领域服务的协调与驱动，为完成某个业务功能，协调领域服务最终交付：
- 基础设施：所有的技术实现细节都被隐藏在基础设施中
- 领域事件：主义+定语，所描述的在领域内的客观事实，公式：产生事件的对象名称+完成的动作的过去式
- 角色命令：由某种角色，触发了某种命令，发生了某种领域事件。
- 问题空间（域）：业务领域中的客观事实以及期望目标的统称。
- 解系统（空间）：解决问题的技术实现。
- 分层架构：用户界面层，应用层，领域层，基础设施层。

![image-20240503170335718](.\images\image-20240503170335718.png)

ddd更适用于业务较为复杂的系统，尤其是分布式系统当中。以上图的流程，我们可以将一个业务垂直拆分为多个环节。

对于某个业务，需要寻找出其中所蕴含的运行逻辑，缕清业务的运行流程。拆分为多个实体，寻找实体间的因果关系，归纳出所谓的子问题。

对于子问题，图中分为三种**核心/支撑/通用**。这里可以以IM中c2c场景为例：

- 核心域：发送&接受信息 —— 业务的核心，项目的核心竞争力所在
- 支撑域：第三方库MQ等 基础设施一类 优先级并不高
- 通用域：中台代码 可横向复用的代码
